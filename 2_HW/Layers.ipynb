{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf272091",
   "metadata": {},
   "source": [
    "Задача 1: BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c537ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Параметры (обучаются)\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        \n",
    "        # Буферы для инференса (не обучаются)\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        # Для backward\n",
    "        self.x_norm = None\n",
    "        self.x_centered = None\n",
    "        self.std_inv = None\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim == 2:\n",
    "            # Полносвязный случай: (N, D)\n",
    "            x = x.T  # (D, N)\n",
    "            axis = 1\n",
    "        elif x.ndim == 4:\n",
    "            # Свёрточный случай: (N, C, H, W) → переставляем в (C, N*H*W)\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.transpose(1, 0, 2, 3).reshape(C, -1)\n",
    "            axis = 1\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input shape\")\n",
    "\n",
    "        if train:\n",
    "            batch_mean = np.mean(x, axis=axis, keepdims=True)  # (C, 1)\n",
    "            batch_var = np.var(x, axis=axis, keepdims=True)    # (C, 1)\n",
    "\n",
    "            # Обновляем running stats\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.flatten()\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.flatten()\n",
    "\n",
    "            self.x_centered = x - batch_mean\n",
    "            self.std_inv = 1.0 / np.sqrt(batch_var + self.eps)\n",
    "            self.x_norm = self.x_centered * self.std_inv\n",
    "\n",
    "            out = self.gamma[:, None] * self.x_norm + self.beta[:, None]\n",
    "        else:\n",
    "            # Используем накопленные средние\n",
    "            x_norm = (x - self.running_mean[:, None]) / np.sqrt(self.running_var[:, None] + self.eps)\n",
    "            out = self.gamma[:, None] * x_norm + self.beta[:, None]\n",
    "\n",
    "        # Возвращаем в исходную форму\n",
    "        if self.input_shape[1] == self.num_features:  # (N, D)\n",
    "            return out.T\n",
    "        else:  # (N, C, H, W)\n",
    "            N, C, H, W = self.input_shape\n",
    "            return out.reshape(C, N, H, W).transpose(1, 0, 2, 3)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.input_shape[1] == self.num_features:\n",
    "            grad_output = grad_output.T  # (D, N)\n",
    "        else:\n",
    "            N, C, H, W = self.input_shape\n",
    "            grad_output = grad_output.transpose(1, 0, 2, 3).reshape(C, -1)\n",
    "\n",
    "        N = grad_output.shape[1]\n",
    "\n",
    "        # Градиент по gamma и beta\n",
    "        dgamma = np.sum(grad_output * self.x_norm, axis=1)\n",
    "        dbeta = np.sum(grad_output, axis=1)\n",
    "\n",
    "        # Градиент по входу\n",
    "        dx_norm = grad_output * self.gamma[:, None]\n",
    "        dvar = np.sum(dx_norm * self.x_centered, axis=1, keepdims=True) * (-0.5) * (self.std_inv ** 3)\n",
    "        dmean = np.sum(dx_norm * (-self.std_inv), axis=1, keepdims=True) + dvar * np.mean(-2.0 * self.x_centered, axis=1, keepdims=True)\n",
    "        dx = dx_norm * self.std_inv + dvar * (2.0 * self.x_centered) / N + dmean / N\n",
    "\n",
    "        # Возвращаем в исходную форму\n",
    "        if self.input_shape[1] == self.num_features:\n",
    "            dx = dx.T\n",
    "        else:\n",
    "            N_orig, C, H, W = self.input_shape\n",
    "            dx = dx.reshape(C, N_orig, H, W).transpose(1, 0, 2, 3)\n",
    "\n",
    "        # Сохраняем градиенты для параметров (если нужен шаг оптимизатора)\n",
    "        self.grad_gamma = dgamma\n",
    "        self.grad_beta = dbeta\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890ac81",
   "metadata": {},
   "source": [
    "Задача 2: Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca215254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "        # Инициализация весов (как в PyTorch: Kaiming Uniform по умолчанию, но для простоты — нормальное распределение)\n",
    "        self.weight = np.random.randn(out_features, in_features) * np.sqrt(2.0 / in_features)\n",
    "        if bias:\n",
    "            self.bias_param = np.zeros(out_features)\n",
    "        else:\n",
    "            self.bias_param = None\n",
    "\n",
    "        self.x = None  # для backward\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  # сохраняем вход для backward\n",
    "        out = x @ self.weight.T\n",
    "        if self.bias:\n",
    "            out += self.bias_param\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # grad_output: (N, out_features)\n",
    "        dx = grad_output @ self.weight  # (N, in_features)\n",
    "        dW = grad_output.T @ self.x      # (out_features, in_features)\n",
    "        db = np.sum(grad_output, axis=0) if self.bias else None\n",
    "\n",
    "        self.grad_weight = dW\n",
    "        if self.bias:\n",
    "            self.grad_bias = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e535f",
   "metadata": {},
   "source": [
    "Задача 3: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c24db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        if not train or self.p == 0:\n",
    "            return x\n",
    "        # Создаём маску: True с вероятностью (1 - p)\n",
    "        self.mask = (np.random.rand(*x.shape) > self.p)\n",
    "        # Масштабируем на (1 - p) для сохранения ожидания\n",
    "        return x * self.mask / (1.0 - self.p)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.mask is None:\n",
    "            return grad_output\n",
    "        return grad_output * self.mask / (1.0 - self.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313fa5a",
   "metadata": {},
   "source": [
    "Задача 4: Активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b06a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa82381",
   "metadata": {},
   "source": [
    "Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbcf004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Устойчивая реализация\n",
    "        self.out = np.where(x >= 0, \n",
    "                            1 / (1 + np.exp(-x)),\n",
    "                            np.exp(x) / (1 + np.exp(x)))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.out * (1 - self.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303473d",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7607cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x, axis=-1):\n",
    "        # Численно устойчивый softmax\n",
    "        x_max = np.max(x, axis=axis, keepdims=True)\n",
    "        exp_x = np.exp(x - x_max)\n",
    "        self.out = exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output, axis=-1):\n",
    "        # grad_output has same shape as self.out\n",
    "        # Softmax Jacobian: J_ik = s_i (δ_ik - s_k)\n",
    "        # => grad_input_i = sum_k grad_output_k * s_i (δ_ik - s_k) = s_i (grad_output_i - sum_k grad_output_k s_k)\n",
    "        s = self.out\n",
    "        sum_s_grad = np.sum(grad_output * s, axis=axis, keepdims=True)\n",
    "        return s * (grad_output - sum_s_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
