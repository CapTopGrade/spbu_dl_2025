{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ffcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import torch\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adaafa",
   "metadata": {},
   "source": [
    "# Класс Node (Узел) для скалярного автоматического дифференцирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b53d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Реализует базовые функции autograd для скалярных значений.\n",
    "    Поддерживает: сложение (+), умножение (*) и функцию активации ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        # Скалярное значение данных\n",
    "        self.data = data\n",
    "        # Градиент (производная целевой функции по этому узлу)\n",
    "        self.grad = 0.0\n",
    "        # Функция для вычисления градиентов этого узла, вызванная из родительского узла\n",
    "        self._backward = lambda: None\n",
    "        # Множество дочерних узлов (входные данные для текущей операции)\n",
    "        self._prev = set(_children)\n",
    "        # Название операции, создавшей этот узел (для отладки/отображения)\n",
    "        self._op = _op\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Возвращает строковое представление узла.\"\"\"\n",
    "        # Используем \"Node\" в выводе, как в примере пользователя.\n",
    "        return f\"Node(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    # --- Операция сложения ---\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Сложение: self + other\"\"\"\n",
    "        # Преобразование скаляра в Node для унификации\n",
    "        other = other if isinstance(other, Node) else Node(other)\n",
    "        \n",
    "        # Создание выходного узла\n",
    "        out = Node(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        # Функция обратного прохода для сложения (d(out)/da = 1, d(out)/db = 1)\n",
    "        def _backward():\n",
    "            # Принцип цепного правила: локальный градиент * градиент родителя\n",
    "            # Применяем += для аккумуляции градиентов (если узел используется в нескольких местах)\n",
    "            self.grad += out.grad * 1.0\n",
    "            other.grad += out.grad * 1.0\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # Поддержка обратного сложения (other + self), например, 5 + Node(10)\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    # --- Операция умножения ---\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Умножение: self * other\"\"\"\n",
    "        other = other if isinstance(other, Node) else Node(other)\n",
    "        \n",
    "        out = Node(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        # Функция обратного прохода для умножения (d(out)/da = b, d(out)/db = a)\n",
    "        def _backward():\n",
    "            # d(out)/d(self) = other.data\n",
    "            self.grad += out.grad * other.data\n",
    "            # d(out)/d(other) = self.data\n",
    "            other.grad += out.grad * self.data\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # Поддержка обратного умножения (other * self), например, 5 * Node(10)\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    # --- Функция активации ReLU ---\n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        Rectified Linear Unit (ReLU)\n",
    "        out = max(0, data)\n",
    "        \"\"\"\n",
    "        out_data = self.data if self.data > 0 else 0.0\n",
    "        out = Node(out_data, (self,), 'relu')\n",
    "\n",
    "        # Функция обратного прохода для ReLU\n",
    "        def _backward():\n",
    "            # d(out)/d(self) = 1, если self.data > 0, иначе 0\n",
    "            self.grad += out.grad * (1.0 if self.data > 0 else 0.0)\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # --- Обратное распространение ошибки ---\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Выполняет обратное распространение градиентов по вычислительному графу.\n",
    "        Используется топологическая сортировка для обеспечения корректного порядка вычислений.\n",
    "        \"\"\"\n",
    "        # 1. Топологическая сортировка узлов графа (обход в глубину)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        \n",
    "        build_topo(self)\n",
    "        \n",
    "        # 2. Инициализация градиента корневого узла (целевой функции)\n",
    "        # d(L)/d(L) = 1\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        # 3. Обратный проход по графу (в обратном топологическом порядке)\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca4024",
   "metadata": {},
   "source": [
    "# МОДУЛЬНЫЕ ТЕСТЫ С ИСПОЛЬЗОВАНИЕМ UNITTEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9b1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAutograd(unittest.TestCase):\n",
    "    \n",
    "    def test_example_from_prompt(self):\n",
    "        \"\"\"Тестирование примера из задания: a + b * c | relu\"\"\"\n",
    "        a = Node(2.0)\n",
    "        b = Node(-3.0)\n",
    "        c = Node(10.0)\n",
    "        \n",
    "        d = a + b * c  # d = 2 + (-3 * 10) = -28\n",
    "        e = d.relu()   # e = max(0, -28) = 0\n",
    "        \n",
    "        # Проверка прямого прохода\n",
    "        self.assertAlmostEqual(e.data, 0.0)\n",
    "        \n",
    "        # Обратный проход\n",
    "        e.backward()\n",
    "        \n",
    "        # dL/dd = 0, т.к. d < 0 после ReLU.\n",
    "        # Все градиенты входных узлов должны быть 0.\n",
    "        \n",
    "        self.assertAlmostEqual(a.grad, 0.0, msg=\"Градиент 'a' должен быть 0 (d < 0 после ReLU)\")\n",
    "        self.assertAlmostEqual(b.grad, 0.0, msg=\"Градиент 'b' должен быть 0 (d < 0 после ReLU)\")\n",
    "        self.assertAlmostEqual(c.grad, 0.0, msg=\"Градиент 'c' должен быть 0 (d < 0 после ReLU)\")\n",
    "        self.assertAlmostEqual(d.grad, 0.0, msg=\"Градиент 'd' должен быть 0 (градиент ReLU при d < 0)\")\n",
    "        self.assertAlmostEqual(e.grad, 1.0, msg=\"Градиент 'e' (корневой узел) должен быть 1\")\n",
    "        \n",
    "        \n",
    "    def test_chain_rule_and_mixed_ops(self):\n",
    "        \"\"\"Тестирование сложной цепи операций (добавление, умножение)\"\"\"\n",
    "        x1 = Node(2.0)\n",
    "        x2 = Node(0.0)\n",
    "        w1 = Node(-3.0)\n",
    "        w2 = Node(1.0)\n",
    "        b = Node(6.8813735870195432) # Имитация смещения\n",
    "        \n",
    "        # Вычисление: L = (x1 * w1) + (x2 * w2) + b\n",
    "        x1w1 = x1 * w1\n",
    "        x2w2 = x2 * w2\n",
    "        x1w1x2w2 = x1w1 + x2w2\n",
    "        n = x1w1x2w2 + b # n = -6.0 + 0.0 + 6.88... = 0.88...\n",
    "        \n",
    "        # ReLU для проверки градиента > 0\n",
    "        o = n.relu() # o = 0.88...\n",
    "        \n",
    "        self.assertAlmostEqual(o.data, 0.8813735870195432)\n",
    "        \n",
    "        o.backward()\n",
    "        \n",
    "        # Проверка градиентов (o.grad=1, n.grad=1, т.к. n > 0):\n",
    "        # dL/db = 1.0\n",
    "        # dL/dw1 = dL/dn * dn/dx1w1 * dx1w1/dw1 = 1.0 * 1.0 * x1.data = 2.0\n",
    "        # dL/dx1 = dL/dn * dn/dx1w1 * dx1w1/dx1 = 1.0 * 1.0 * w1.data = -3.0\n",
    "        # dL/dw2 = dL/dn * dn/dx2w2 * dx2w2/dw2 = 1.0 * 1.0 * x2.data = 0.0\n",
    "        # dL/dx2 = dL/dn * dn/dx2w2 * dx2w2/dx2 = 1.0 * 1.0 * w2.data = 1.0 (НЕ 0.0!)\n",
    "        \n",
    "        self.assertAlmostEqual(b.grad, 1.0)\n",
    "        self.assertAlmostEqual(w1.grad, 2.0)\n",
    "        self.assertAlmostEqual(x1.grad, -3.0)\n",
    "        self.assertAlmostEqual(w2.grad, 0.0) \n",
    "        self.assertAlmostEqual(x2.grad, 1.0, msg=\"Градиент x2 должен быть 1.0 (w2.data), а не 0.0\")\n",
    "\n",
    "\n",
    "    def test_relu_negative_input(self):\n",
    "        \"\"\"Тестирование ReLU для отрицательного входного значения\"\"\"\n",
    "        x = Node(-5.0)\n",
    "        y = Node(10.0)\n",
    "        \n",
    "        # Результат = max(0, -5) * 10 = 0\n",
    "        a = x.relu()\n",
    "        b = a * y\n",
    "        \n",
    "        self.assertAlmostEqual(a.data, 0.0)\n",
    "        self.assertAlmostEqual(b.data, 0.0)\n",
    "        \n",
    "        b.backward()\n",
    "        \n",
    "        # dL/dx = 0, т.к. x < 0.\n",
    "        self.assertAlmostEqual(y.grad, 0.0) # dL/dy = a.data * dL/db = 0 * 1 = 0\n",
    "        self.assertAlmostEqual(a.grad, 10.0)\n",
    "        self.assertAlmostEqual(x.grad, 0.0)\n",
    "\n",
    "\n",
    "    def test_pytorch_validation(self):\n",
    "        \"\"\"Валидация градиентов с помощью PyTorch (для ground truth)\"\"\"\n",
    "        # Инициализация для Node\n",
    "        a = Node(3.0)\n",
    "        b = Node(4.0)\n",
    "        c = Node(-2.0)\n",
    "        \n",
    "        # Инициализация для PyTorch\n",
    "        # requires_grad=True для отслеживания операций\n",
    "        ta = torch.tensor([3.0], requires_grad=True)\n",
    "        tb = torch.tensor([4.0], requires_grad=True)\n",
    "        tc = torch.tensor([-2.0], requires_grad=True)\n",
    "\n",
    "        # Вычисление в Node: L = ReLU((a * b) + c)\n",
    "        ab = a * b\n",
    "        abc = ab + c\n",
    "        L = abc.relu()\n",
    "        L.backward()\n",
    "\n",
    "        # Вычисление в PyTorch: L_torch = ReLU((ta * tb) + tc)\n",
    "        tabc = (ta * tb) + tc\n",
    "        L_torch = torch.relu(tabc)\n",
    "        L_torch.backward()\n",
    "\n",
    "        # Проверка градиентов\n",
    "        self.assertAlmostEqual(L.data, L_torch.item()) # L = 10.0\n",
    "\n",
    "        # Сравнение градиентов:\n",
    "        # dL/da = 4.0\n",
    "        # dL/db = 3.0\n",
    "        # dL/dc = 1.0\n",
    "        self.assertAlmostEqual(a.grad, ta.grad.item(), places=5, msg=\"Градиент 'a' не совпадает\")\n",
    "        self.assertAlmostEqual(b.grad, tb.grad.item(), places=5, msg=\"Градиент 'b' не совпадает\")\n",
    "        self.assertAlmostEqual(c.grad, tc.grad.item(), places=5, msg=\"Градиент 'c' не совпадает\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c7608",
   "metadata": {},
   "source": [
    "# ПРИМЕР ИСПОЛЬЗОВАНИЯ ИЗ ЗАДАНИЯ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90aafa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_chain_rule_and_mixed_ops (__main__.TestAutograd.test_chain_rule_and_mixed_ops)\n",
      "Тестирование сложной цепи операций (добавление, умножение) ... ok\n",
      "test_example_from_prompt (__main__.TestAutograd.test_example_from_prompt)\n",
      "Тестирование примера из задания: a + b * c | relu ... ok\n",
      "test_pytorch_validation (__main__.TestAutograd.test_pytorch_validation)\n",
      "Валидация градиентов с помощью PyTorch (для ground truth) ... ok\n",
      "test_relu_negative_input (__main__.TestAutograd.test_relu_negative_input)\n",
      "Тестирование ReLU для отрицательного входного значения ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Запуск Примера Пользователя ---\n",
      "Input: a=2, b=-3, c=10\n",
      "Operation: d = a + b * c, e = d.relu()\n",
      "\n",
      "Output (после e.backward()):\n",
      "Node(data=2, grad=0.0)\n",
      "Node(data=-3, grad=0.0)\n",
      "Node(data=10, grad=0.0)\n",
      "Node(data=-28, grad=0.0)\n",
      "Node(data=0.0, grad=1.0)\n",
      "\n",
      "--- Запуск Модульных Тестов Unittest ---\n",
      "\n",
      "--- Пример с положительным ReLU ---\n",
      "L.data: 3.0, L.grad: 1.0\n",
      "x.grad: 1.0\n",
      "y.grad: 1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"--- Запуск Примера Пользователя ---\")\n",
    "    \n",
    "    # Сброс градиентов для чистоты (при запуске как main)\n",
    "    def reset_grads(*nodes):\n",
    "        for node in nodes:\n",
    "            node.grad = 0.0\n",
    "            \n",
    "    a = Node(2)\n",
    "    b = Node(-3)\n",
    "    c = Node(10)\n",
    "    \n",
    "    # Сохраняем исходные данные для печати, как в примере\n",
    "    a_data = a.data\n",
    "    b_data = b.data\n",
    "    c_data = c.data\n",
    "    \n",
    "    d = a + b * c # d = 2 + (-3 * 10) = -28\n",
    "    e = d.relu()  # e = max(0, -28) = 0\n",
    "    \n",
    "    e.backward()\n",
    "    \n",
    "    print(f\"Input: a={a_data}, b={b_data}, c={c_data}\")\n",
    "    print(f\"Operation: d = a + b * c, e = d.relu()\")\n",
    "    \n",
    "    print(\"\\nOutput (после e.backward()):\")\n",
    "    \n",
    "    # Выводим фактические корректные результаты:\n",
    "    # Градиенты a, b, c, d равны 0.0, т.к. градиент ReLU для отрицательного входа равен 0.\n",
    "    print(a) \n",
    "    print(b) \n",
    "    print(c) \n",
    "    print(d) \n",
    "    print(e) # Корневой узел, grad = 1.0\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    print(\"\\n--- Запуск Модульных Тестов Unittest ---\")\n",
    "    \n",
    "    # Используем TestLoader.loadTestsFromTestCase() для избежания DeprecationWarning\n",
    "    loader = unittest.TestLoader()\n",
    "    suite = loader.loadTestsFromTestCase(TestAutograd)\n",
    "    \n",
    "    # Запуск тестов\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(suite)\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    print(\"\\n--- Пример с положительным ReLU ---\")\n",
    "    reset_grads(a, b, c)\n",
    "    x = Node(5.0)\n",
    "    y = Node(-2.0)\n",
    "    z = x + y * Node(1.0) # z = 5 + (-2) * 1 = 3\n",
    "    L = z.relu() # L = 3\n",
    "    L.backward()\n",
    "    \n",
    "    print(f\"L.data: {L.data}, L.grad: {L.grad}\")\n",
    "    print(f\"x.grad: {x.grad}\") # Должно быть 1.0\n",
    "    print(f\"y.grad: {y.grad}\") # Должно быть 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
